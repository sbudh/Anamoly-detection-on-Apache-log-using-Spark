{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ce273c-3037-435e-9aa9-efa269b40c66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, max, min, sum, desc, from_unixtime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"API Logs Anomaly Detection\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cef9979b-ba17-4b8a-a71a-50bba4f657fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema for Apache Access logs\n",
    "schema = StructType([\n",
    "    StructField(\"ip\", StringType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"method\", StringType(), True),\n",
    "    StructField(\"endpoint\", StringType(), True),\n",
    "    StructField(\"protocol\", StringType(), True),\n",
    "    StructField(\"response_code\", IntegerType(), True),\n",
    "    StructField(\"content_size\", LongType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c2c3033-37e6-46bf-933e-d7ebda45d3ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+------------------+------------------+\n|min_content_size|max_content_size|count_content_size|total_content_size|\n+----------------+----------------+------------------+------------------+\n|             100|           10000|            100000|         504987157|\n+----------------+----------------+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Load data from parquet file\n",
    "logs_df = spark.read.schema(schema).parquet(\"/FileStore/tables/apache_logs__1_.parquet\")\n",
    "\n",
    "# Convert timestamp to human-readable format\n",
    "logs_df = logs_df.withColumn(\"timestamp\", from_unixtime(col(\"timestamp\")))\n",
    "\n",
    "# Content Size Statistics\n",
    "content_stats = logs_df.agg(\n",
    "    min(\"content_size\").alias(\"min_content_size\"),\n",
    "    max(\"content_size\").alias(\"max_content_size\"),\n",
    "    count(\"content_size\").alias(\"count_content_size\"),\n",
    "    sum(\"content_size\").alias(\"total_content_size\")\n",
    ")\n",
    "\n",
    "content_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac44dae7-4afa-41ba-88d4-0386569a1ae5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n|endpoint|sum(content_size)|\n+--------+-----------------+\n|    null|        504987157|\n+--------+-----------------+\n\n+----+-----------------+\n| day|sum(content_size)|\n+----+-----------------+\n|null|        504987157|\n+----+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Top Endpoints by Content Size\n",
    "top_endpoints = logs_df.groupBy(\"endpoint\").sum(\"content_size\").alias(\"total_content_size\").orderBy(desc(\"sum(content_size)\"))\n",
    "top_endpoints.show(10)\n",
    "\n",
    "# Daily Content Size Statistics\n",
    "daily_content_size = logs_df.groupBy(from_unixtime(col(\"timestamp\"), \"yyyy-MM-dd\").alias(\"day\")).sum(\"content_size\").alias(\"daily_content_size\").orderBy(\"day\")\n",
    "daily_content_size.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b1ddc1a-e207-4ab5-adb9-dc77308cc6e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n| day|sum(content_size)|\n+----+-----------------+\n|null|        504987157|\n+----+-----------------+\n\n+--------+------+\n|endpoint| count|\n+--------+------+\n|    null|100000|\n+--------+------+\n\n+----+------+\n|  ip| count|\n+----+------+\n|null|100000|\n+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Daily Content Size Statistics\n",
    "daily_content_size = logs_df.groupBy(from_unixtime(col(\"timestamp\"), \"yyyy-MM-dd\").alias(\"day\")).sum(\"content_size\").alias(\"daily_content_size\").orderBy(\"day\")\n",
    "daily_content_size.show()\n",
    "\n",
    "# Top Visited Endpoints\n",
    "#from pyspark.sql.functions import desc\n",
    "\n",
    "# Top Visited Endpoints\n",
    "top_visited_endpoints = logs_df.groupBy(\"endpoint\").count().alias(\"visit_count\").orderBy(desc(\"count\"))\n",
    "top_visited_endpoints.show(10)\n",
    "\n",
    "# Frequent Visitors\n",
    "frequent_visitors = logs_df.groupBy(\"ip\").count().alias(\"visit_count\").orderBy(desc(\"count\"))\n",
    "frequent_visitors.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2bb9914-6d2b-4398-85ed-cb8786ae863b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n|response_code| count|\n+-------------+------+\n|         null|100000|\n+-------------+------+\n\n+----+------+\n|  ip| count|\n+----+------+\n|null|100000|\n+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "## Response Code Analysis\n",
    "response_code_analysis = logs_df.groupBy(\"response_code\").count().alias(\"count\").orderBy(desc(\"count\"))\n",
    "response_code_analysis.show()\n",
    "\n",
    "# IPs accessing the server more than 10 times\n",
    "frequent_ip_accesses = logs_df.groupBy(\"ip\").count().alias(\"visit_count\").filter(col(\"count\") > 10).orderBy(desc(\"count\"))\n",
    "frequent_ip_accesses.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4deef97f-b228-4780-b6c7-388e17552792",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n|timestamp|endpoint|\n+---------+--------+\n+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Top 10 latest 404 requests\n",
    "latest_404_requests = logs_df.filter(col(\"response_code\") == 404).orderBy(desc(\"timestamp\")).select(\"timestamp\", \"endpoint\").limit(10)\n",
    "latest_404_requests.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a90908d6-efc8-4e31-8f76-09b77e4cee2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c5613f7-d61a-47d3-9f11-7bd248605e8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Anamoly detection in API logs using spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
